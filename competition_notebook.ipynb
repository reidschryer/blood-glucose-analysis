{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the dataset:\n",
    "\n",
    "This dataset was from a [Kaggle Competition](https://www.kaggle.com/competitions/brist1d) that ran in Fall 2024. It describes blood glucose levels, measured in mmol/L, over the past 6 hours in a tabular format with 5 minute increments. Additionally, the dataset includes information on insulin doses received, carbohydrates consumed, mean heart rate, steps taken, calories burnt, and self-declared activity performed. \n",
    "\n",
    "The target variable was to predict blood glucose levels 1 hour into the future, given this past data from the past 6 hours. \n",
    "\n",
    "Data cleaning was necessary due to the large number of missing data points. Feature engineering was also used to better reflect a continuous time series with both micro and macro trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro/data imports\n",
    "\n",
    "Below is a full notebook detailing my submission for the BrisT1D Blood Glucose Prediction Competition, hosted by Kaggle. Datasets are available on [Kaggle](https://www.kaggle.com/competitions/brist1d/overview) - place all CSV files in a directory named 'brist1d' - left empty for demonstrative purposes. The following python packages are used in this notebook:\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Matplotlib\n",
    "- Scikit-Learn\n",
    "- XGBoost\n",
    "- CatBoost\n",
    "- Pytorch\n",
    "- Weights & Biases (optional, used to tune hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GroupKFold\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('brist1d/train.csv',\n",
    "                        index_col='id', \n",
    "                        parse_dates=['time'])\n",
    "\n",
    "df_test = pd.read_csv('brist1d/test.csv', \n",
    "                       index_col='id', \n",
    "                       parse_dates=['time'])\n",
    "\n",
    "df_subm = pd.read_csv('brist1d/sample_submission.csv',\n",
    "                       index_col='id')\n",
    "\n",
    "df_train.columns = df_train.columns.str.replace(':', '-')\n",
    "df_test.columns = df_test.columns.str.replace(':', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe memory reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of df_train before reduction: 686.10 MB\n",
      "Memory usage of df_train after reduction: 307.48 MB\n"
     ]
    }
   ],
   "source": [
    "def reduceMemory(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')\n",
    "        elif pd.api.types.is_object_dtype(df[col]):\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(f\"Memory usage of df_train before reduction: {df_train.memory_usage().sum() / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "df_train = reduceMemory(df_train)\n",
    "df_test = reduceMemory(df_test)\n",
    "\n",
    "print(f\"Memory usage of df_train after reduction: {df_train.memory_usage().sum() / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and feature engineering\n",
    "\n",
    "Due to the time-series lags in this dataset, NaNs are often propagated along multiple rows that are actually the same value. To ensure similar filling, I used row interpolation to fill the data based on nearby neighbors.\n",
    "\n",
    "Additionally, time series lags exist up to 6 hours into the past for this dataset. I found that including more than 120 minutes of data did not improve model performance, so below also includes limiting columns to the latest 24 (5 min x 24 = 120 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = df_train.columns\n",
    "\n",
    "bg_cols = [col for col in columns_list if col.startswith('bg-0') or col.startswith('bg-1')]\n",
    "cals_cols = [col for col in columns_list if col.startswith('cals-0') or col.startswith('cals-1')]\n",
    "carbs_cols = [col for col in columns_list if col.startswith('carbs-0') or col.startswith('carbs-1')]\n",
    "insulin_cols = [col for col in columns_list if col.startswith('insulin-0') or col.startswith('insulin-1')]\n",
    "hr_cols = [col for col in columns_list if col.startswith('hr-0') or col.startswith('hr-1')]\n",
    "steps_cols = [col for col in columns_list if col.startswith('steps-0') or col.startswith('steps-1')]\n",
    "\n",
    "for cols in [bg_cols, cals_cols, carbs_cols, insulin_cols, hr_cols, steps_cols]:\n",
    "    df_train[cols] = df_train[cols].interpolate(axis=1, limit_direction='both')\n",
    "    df_test[cols] = df_test[cols].interpolate(axis=1, limit_direction='both')\n",
    "\n",
    "# Carbs, steps, and cals likely have NaNs meaning 0, not missing information\n",
    "df_train[cals_cols] = df_train[cals_cols].fillna(0)\n",
    "df_test[cals_cols] = df_test[cals_cols].fillna(0)\n",
    "\n",
    "df_train[carbs_cols] = df_train[carbs_cols].fillna(0)\n",
    "df_test[carbs_cols] = df_test[carbs_cols].fillna(0)\n",
    "\n",
    "df_train[steps_cols] = df_train[steps_cols].fillna(0)\n",
    "df_test[steps_cols] = df_test[steps_cols].fillna(0)\n",
    "\n",
    "feature_cols = bg_cols + cals_cols + carbs_cols + insulin_cols + hr_cols + steps_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally impute remaining features based on means of their respective columns. I tried multiple more complicated imputing methods including splines and iterative imputing, but neither gave a performance improvement to the model. KNN imputation was too computationally expensive for the number of rows in the dataset (due to the nonlinear time complexity of searching for the closest neighbors/rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer()\n",
    "\n",
    "df_train[feature_cols] = mean_imputer.fit_transform(df_train[feature_cols])\n",
    "df_test[feature_cols] = mean_imputer.transform(df_test[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "For `carbs`, `steps`, and `cals`, I wanted to add running cumulative summations across the 2 hour window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[[f'cumsum_{col}' for col in cals_cols]] = df_train[[col for col in cals_cols]].cumsum(axis=1)\n",
    "df_test[[f'cumsum_{col}' for col in cals_cols]] = df_test[[col for col in cals_cols]].cumsum(axis=1)\n",
    "\n",
    "df_train[[f'cumsum_{col}' for col in carbs_cols]] = df_train[[col for col in carbs_cols]].cumsum(axis=1)\n",
    "df_test[[f'cumsum_{col}' for col in carbs_cols]] = df_test[[col for col in carbs_cols]].cumsum(axis=1)\n",
    "\n",
    "df_train[[f'cumsum_{col}' for col in steps_cols]] = df_train[[col for col in steps_cols]].cumsum(axis=1)\n",
    "df_test[[f'cumsum_{col}' for col in steps_cols]] = df_test[[col for col in steps_cols]].cumsum(axis=1)\n",
    "\n",
    "cumsum_cols = [col for col in df_train.columns if col.startswith('cumsum')]\n",
    "\n",
    "feature_cols.extend(cumsum_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sin and cos values for the hour (for example to ensure 11pm smoothly translates to 12am from the `time` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sin_hour'] = np.sin(np.pi * df_train['time'].dt.hour / 12)\n",
    "df_train['cos_hour'] = np.cos(np.pi * df_train['time'].dt.hour / 12)\n",
    "\n",
    "df_test['sin_hour'] = np.sin(np.pi * df_test['time'].dt.hour / 12)\n",
    "df_test['cos_hour'] = np.cos(np.pi * df_test['time'].dt.hour / 12)\n",
    "\n",
    "feature_cols.extend(['sin_hour', 'cos_hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add different windows of means and variances for blood glucose, to capture any significance in variance on predictability of future blood glucose levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [3, 6, 9]:\n",
    "    df_train[f'bg_mean_{window}'] = df_train[bg_cols[-window:]].mean(axis=1)\n",
    "    df_train[f'bg_var_{window}'] = df_train[bg_cols[-window:]].var(axis=1)\n",
    "    df_test[f'bg_mean_{window}'] = df_test[bg_cols[-window:]].mean(axis=1)\n",
    "    df_test[f'bg_var_{window}'] = df_test[bg_cols[-window:]].var(axis=1)\n",
    "    feature_cols.extend([f'bg_mean_{window}', f'bg_var_{window}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to include the column `activity` using a target encoder or similar. However, most activities appeared less than 20 times in the training dataset. Furthermore, of those who appeared more frequently in the training dataset, they did not show up enough in the test dataset to be worth including.\n",
    "\n",
    "Final preparations include using a standard scaler and splitting the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "\n",
    "df_train[feature_cols] = std_scaler.fit_transform(df_train[feature_cols])\n",
    "df_test[feature_cols] = std_scaler.transform(df_test[feature_cols])\n",
    "\n",
    "X_train = df_train[feature_cols].copy()\n",
    "y_train = df_train['bg+1-00'].copy()\n",
    "\n",
    "X_test = df_test[feature_cols].copy()\n",
    "\n",
    "stratify_groups = df_train['p_num'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Gradient Boosting Decision Trees in a Stacking Regressor\n",
    "\n",
    "An easy start for regressions on tabular data are GBDTs - I used XGBoost and CatBoost as an ensemble for this dataset. For hyperparameter tuning, I used bayesian hyperparameter optimization, specifically employing [Weights & Biases](https://wandb.ai/site/) for the informative visualization and tracking of results. Out of sample validation was performed to reduce overfitting.\n",
    "\n",
    "Below is the full parameter grid used. For purposes of keeping this notebook consolidated, below is the function used to tune the hyperparameters. Ensure you have `wandb` installed and the final line is uncommented if you choose to run the tuning yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'rmse', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'xgb_n_estimators': {'min': 1000, 'max': 4000},\n",
    "        'xgb_learning_rate': {'min': 0.005, 'max': 0.2},\n",
    "        'xgb_max_depth': {'min': 4, 'max': 8},\n",
    "        'catboost_iterations': {'min': 1000, 'max': 4000},\n",
    "        'catboost_depth': {'min': 4, 'max': 8}\n",
    "    }\n",
    "}\n",
    "\n",
    "def train_model():\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators = config.xgb_n_estimators,\n",
    "        learning_rate= config.xgb_learning_rate,\n",
    "        max_depth = config.xgb_max_depth,\n",
    "        random_state = RANDOM_SEED,\n",
    "        eval_metric = 'rmse'\n",
    "    )\n",
    "\n",
    "    catboost = CatBoostRegressor(\n",
    "        iterations = config.catboost_iterations,\n",
    "        depth = config.catboost_depth,\n",
    "        random_seed = RANDOM_SEED,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    estimators = [\n",
    "        ('xgb', xgb),\n",
    "        ('catboost', catboost)\n",
    "    ]\n",
    "\n",
    "    final_estimator = RidgeCV(alphas=[1e-2, 1e-1, 1])\n",
    "\n",
    "    stacking_model = StackingRegressor(\n",
    "        estimators = estimators, \n",
    "        final_estimator = final_estimator,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    X_split, X_val, y_split, y_val = train_test_split(X_train, y_train)\n",
    "\n",
    "    stacking_model.fit(X_split, y_split)\n",
    "    y_pred = stacking_model.predict(X_val)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    wandb.log({'rmse': rmse,\n",
    "               'ridge_alpha': stacking_model.final_estimator_.alpha_})\n",
    "\n",
    "def main():\n",
    "    sweep_id = wandb.sweep(sweep_config, project='kaggle-bg-gbdt-sweep')\n",
    "\n",
    "    wandb.agent(sweep_id, function=train_model, count=50)\n",
    "\n",
    "# # Uncomment to run:\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the visualization of best parameters after 13 runs. The best out-of-sample RMSE remained at around 1.50.\n",
    "\n",
    "![img](wandb_chart.png)\n",
    "\n",
    "The best hyperparemters for this stacking regressor are as follows:\n",
    "\n",
    "- Catboost depth: 8\n",
    "- Catboost iterations: 1800\n",
    "- XGBoost learning rate: 0.15\n",
    "- XGBoost max depth: 8\n",
    "- XGBoost number of estimators: 2800\n",
    "- Final estimator (RidgeCV) alpha: 1.0\n",
    "\n",
    "Final parameters after decided by the Weights & Biases search are used in the models below to fit the full dataset. I additionally used a 5-fold cross validation on the best hyperparameter set to understand the range of RMSE scores (using +/- 2 times the standard deviation) \n",
    "\n",
    "The model with the tuned parameters is then used to predict the `bg+1:00` column in the test dataset, which is then ready for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 2.1326 (+/- 0.2112)\n"
     ]
    }
   ],
   "source": [
    "xgb_final = XGBRegressor(\n",
    "    n_estimators = 2800,\n",
    "    learning_rate = 0.15,\n",
    "    max_depth = 8,\n",
    "    random_state = RANDOM_SEED,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "catboost_final = CatBoostRegressor(\n",
    "    iterations = 1800,\n",
    "    depth = 8,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', xgb_final),\n",
    "    ('catboost', catboost_final)\n",
    "]\n",
    "\n",
    "final_estimator = Ridge(alpha=1.0)\n",
    "\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators, \n",
    "    final_estimator=final_estimator,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    stacking_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    groups=stratify_groups,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"CV RMSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_gbdt = stacking_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Multilayer perceptron neural network with tuned regularization\n",
    "\n",
    "This model was inspired by the analysis of the paper [\"Well-tuned Simple Nets Excel on Tabular Datasets\"](https://arxiv.org/abs/2106.11189) by Kadra et. all, which I came across when doing research on the effectiveness on TabNets, a popular technique for tabular learning.\n",
    "\n",
    "The authors findings were simple - using a multilayer perceptron neural network with a \"cocktail\" of regularization features, these neural networks can perform better than GBDTs on many tabular datasets. I wanted to try to implement my own python class around some of these regularization techniques, in hopes to refine and add more regularization techniques in the future. The full model architecture is in `MLP_Regressor.py`, and is called in this file for tuning and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_Regressor import MLPRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# convert to float32 for MPS torch support\n",
    "X_split, X_val, y_split, y_val = train_test_split(X_train.astype(np.float32).values, y_train.astype(np.float32).values, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# I default to mps as I use an Apple silicon macbook\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_split_tensor = torch.from_numpy(X_split).to(device)\n",
    "y_split_tensor = torch.from_numpy(y_split).to(device)\n",
    "X_val_tensor = torch.from_numpy(X_val).to(device)\n",
    "y_val_tensor = torch.from_numpy(y_val).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_split_tensor, y_split_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell defines the training loop for this MLP Regressor. While I don't have the compute power to tune hyperparameters of a large neural network, a tool like [Weights & Biases](https://wandb.ai/site/) or [Optuna](https://optuna.org) use SOTA algorithms to efficiently find the best regularization cocktail. The below parameters provide reasonable RMSE results, though could be improved with hyperparameter optimization. \n",
    "\n",
    "Due to the option of snapshot ensembles, models are saved to the folder `best_models` - when loading these models, be sure to run inputs through each model and take the mean output to effectively leverage the ensemble. \n",
    "\n",
    "To go more in depth on the architectural choices for this model, the MLP Regressor class is meant to work closely with the below training loop. This is evident through optimizer and scheduler initialization done through the MLP class, as an AdamW optimizer and Cosine Annealing scheduler are paramount for weight decay and snapshot ensembling, respectively. I aimed to remove hyperparameter tuning from the training function to simplify parameter tuning (despite the need for snapshot ensembles or early stopping to be called during the training loop), so I created them as parameters in the model that are then called by the training loop.\n",
    "\n",
    "`ensemble_load` and `ensemble_predict` in the MLP Regressor class are helper functions for the snapshot ensemble process - when the cosine annealer resets its cycle, I only save the `state_dict` of the final model in the cycle. `ensemble_load` is used to initialize models with the same parameters and state_dict as they were saved from. `ensemble_predict` uses these models to predict a given X input, helpful for validation testing of the OOS data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/210 | Train loss: 6.5576 | Val loss: 4.4824 | Val RMSE: 2.1172\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "\tValidation loss improved\n",
      "Epoch 2/210 | Train loss: 5.0056 | Val loss: 4.2806 | Val RMSE: 2.0690\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "\tValidation loss improved\n",
      "Epoch 3/210 | Train loss: 4.6710 | Val loss: 4.1994 | Val RMSE: 2.0493\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "\tValidation loss improved\n",
      "Epoch 4/210 | Train loss: 4.5246 | Val loss: 4.2509 | Val RMSE: 2.0618\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 5/210 | Train loss: 4.4213 | Val loss: 4.1208 | Val RMSE: 2.0300\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "\tValidation loss improved\n",
      "Epoch 6/210 | Train loss: 4.3769 | Val loss: 4.1370 | Val RMSE: 2.0340\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 7/210 | Train loss: 4.3361 | Val loss: 4.0759 | Val RMSE: 2.0189\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "\tValidation loss improved\n",
      "Epoch 8/210 | Train loss: 4.3176 | Val loss: 4.1511 | Val RMSE: 2.0374\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 9/210 | Train loss: 4.2936 | Val loss: 4.0607 | Val RMSE: 2.0151\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "\tValidation loss improved\n",
      "Epoch 10/210 | Train loss: 4.2572 | Val loss: 4.0006 | Val RMSE: 2.0002\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "\tValidation loss improved\n",
      "Epoch 11/210 | Train loss: 4.2316 | Val loss: 4.0066 | Val RMSE: 2.0016\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 12/210 | Train loss: 4.2108 | Val loss: 3.9786 | Val RMSE: 1.9946\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "\tValidation loss improved\n",
      "Epoch 13/210 | Train loss: 4.1704 | Val loss: 3.9601 | Val RMSE: 1.9900\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "\tValidation loss improved\n",
      "Epoch 14/210 | Train loss: 4.1370 | Val loss: 3.8520 | Val RMSE: 1.9626\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "\tValidation loss improved\n",
      "Epoch 15/210 | Train loss: 4.0990 | Val loss: 3.8437 | Val RMSE: 1.9605\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "\tValidation loss improved\n",
      "Epoch 16/210 | Train loss: 4.0566 | Val loss: 3.8268 | Val RMSE: 1.9562\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "\tValidation loss improved\n",
      "Epoch 17/210 | Train loss: 4.0348 | Val loss: 3.8138 | Val RMSE: 1.9529\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "\tValidation loss improved\n",
      "Epoch 18/210 | Train loss: 3.9904 | Val loss: 3.8094 | Val RMSE: 1.9518\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "\tValidation loss improved\n",
      "Epoch 19/210 | Train loss: 3.9654 | Val loss: 3.7819 | Val RMSE: 1.9447\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "\tValidation loss improved\n",
      "Epoch 20/210 | Train loss: 3.9109 | Val loss: 3.7539 | Val RMSE: 1.9375\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "\tValidation loss improved\n",
      "Epoch 21/210 | Train loss: 3.8612 | Val loss: 3.7150 | Val RMSE: 1.9274\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "\tValidation loss improved\n",
      "Epoch 22/210 | Train loss: 3.8323 | Val loss: 3.7025 | Val RMSE: 1.9242\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "\tValidation loss improved\n",
      "Epoch 23/210 | Train loss: 3.7912 | Val loss: 3.6880 | Val RMSE: 1.9204\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "\tValidation loss improved\n",
      "Epoch 24/210 | Train loss: 3.7481 | Val loss: 3.6733 | Val RMSE: 1.9166\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "\tValidation loss improved\n",
      "Epoch 25/210 | Train loss: 3.7277 | Val loss: 3.6652 | Val RMSE: 1.9145\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "\tValidation loss improved\n",
      "Epoch 26/210 | Train loss: 3.6888 | Val loss: 3.6566 | Val RMSE: 1.9122\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "\tValidation loss improved\n",
      "Epoch 27/210 | Train loss: 3.6353 | Val loss: 3.6416 | Val RMSE: 1.9083\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "\tValidation loss improved\n",
      "Epoch 28/210 | Train loss: 3.6246 | Val loss: 3.6546 | Val RMSE: 1.9117\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "Epoch 29/210 | Train loss: 3.6126 | Val loss: 3.6620 | Val RMSE: 1.9136\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "Epoch 30/210 | Train loss: 3.6125 | Val loss: 3.6537 | Val RMSE: 1.9115\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n",
      "Epoch 31/210 | Train loss: 4.1497 | Val loss: 4.1931 | Val RMSE: 2.0477\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "Epoch 32/210 | Train loss: 4.2063 | Val loss: 4.0126 | Val RMSE: 2.0031\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "Epoch 33/210 | Train loss: 4.2298 | Val loss: 4.0900 | Val RMSE: 2.0224\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "Epoch 34/210 | Train loss: 4.2456 | Val loss: 4.0135 | Val RMSE: 2.0034\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 35/210 | Train loss: 4.2222 | Val loss: 4.0928 | Val RMSE: 2.0231\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "Epoch 36/210 | Train loss: 4.2119 | Val loss: 4.0100 | Val RMSE: 2.0025\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 37/210 | Train loss: 4.2220 | Val loss: 3.9982 | Val RMSE: 1.9996\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "Epoch 38/210 | Train loss: 4.1983 | Val loss: 4.0340 | Val RMSE: 2.0085\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 39/210 | Train loss: 4.1658 | Val loss: 4.0069 | Val RMSE: 2.0017\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "Epoch 40/210 | Train loss: 4.1576 | Val loss: 3.9182 | Val RMSE: 1.9794\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "Epoch 41/210 | Train loss: 4.1327 | Val loss: 3.9056 | Val RMSE: 1.9763\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 42/210 | Train loss: 4.1007 | Val loss: 3.8937 | Val RMSE: 1.9733\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "Epoch 43/210 | Train loss: 4.0832 | Val loss: 3.8740 | Val RMSE: 1.9682\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "Epoch 44/210 | Train loss: 4.0328 | Val loss: 3.8217 | Val RMSE: 1.9549\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "Epoch 45/210 | Train loss: 4.0223 | Val loss: 3.8057 | Val RMSE: 1.9508\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "Epoch 46/210 | Train loss: 3.9910 | Val loss: 3.7771 | Val RMSE: 1.9435\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "Epoch 47/210 | Train loss: 3.9541 | Val loss: 3.7492 | Val RMSE: 1.9363\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "Epoch 48/210 | Train loss: 3.9230 | Val loss: 3.7467 | Val RMSE: 1.9356\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "Epoch 49/210 | Train loss: 3.8891 | Val loss: 3.7064 | Val RMSE: 1.9252\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "Epoch 50/210 | Train loss: 3.8458 | Val loss: 3.6972 | Val RMSE: 1.9228\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "Epoch 51/210 | Train loss: 3.7980 | Val loss: 3.6726 | Val RMSE: 1.9164\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "Epoch 52/210 | Train loss: 3.7456 | Val loss: 3.6532 | Val RMSE: 1.9113\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "Epoch 53/210 | Train loss: 3.7045 | Val loss: 3.6412 | Val RMSE: 1.9082\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "\tValidation loss improved\n",
      "Epoch 54/210 | Train loss: 3.6663 | Val loss: 3.6125 | Val RMSE: 1.9007\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "\tValidation loss improved\n",
      "Epoch 55/210 | Train loss: 3.6177 | Val loss: 3.6042 | Val RMSE: 1.8985\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "\tValidation loss improved\n",
      "Epoch 56/210 | Train loss: 3.5819 | Val loss: 3.5933 | Val RMSE: 1.8956\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "\tValidation loss improved\n",
      "Epoch 57/210 | Train loss: 3.5750 | Val loss: 3.5894 | Val RMSE: 1.8946\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "\tValidation loss improved\n",
      "Epoch 58/210 | Train loss: 3.5441 | Val loss: 3.5966 | Val RMSE: 1.8965\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "Epoch 59/210 | Train loss: 3.5420 | Val loss: 3.5853 | Val RMSE: 1.8935\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "\tValidation loss improved\n",
      "Epoch 60/210 | Train loss: 3.5429 | Val loss: 3.5920 | Val RMSE: 1.8953\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n",
      "Epoch 61/210 | Train loss: 4.1326 | Val loss: 3.9967 | Val RMSE: 1.9992\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "Epoch 62/210 | Train loss: 4.1923 | Val loss: 4.0304 | Val RMSE: 2.0076\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "Epoch 63/210 | Train loss: 4.1958 | Val loss: 4.1942 | Val RMSE: 2.0480\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "Epoch 64/210 | Train loss: 4.2078 | Val loss: 4.0377 | Val RMSE: 2.0094\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 65/210 | Train loss: 4.2002 | Val loss: 4.0794 | Val RMSE: 2.0197\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "Epoch 66/210 | Train loss: 4.1919 | Val loss: 4.0840 | Val RMSE: 2.0209\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 67/210 | Train loss: 4.1855 | Val loss: 4.0148 | Val RMSE: 2.0037\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "Epoch 68/210 | Train loss: 4.1768 | Val loss: 4.0034 | Val RMSE: 2.0008\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 69/210 | Train loss: 4.1414 | Val loss: 4.0571 | Val RMSE: 2.0142\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "Epoch 70/210 | Train loss: 4.1361 | Val loss: 3.9222 | Val RMSE: 1.9804\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "Epoch 71/210 | Train loss: 4.1285 | Val loss: 3.9057 | Val RMSE: 1.9763\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 72/210 | Train loss: 4.0941 | Val loss: 3.8553 | Val RMSE: 1.9635\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "Epoch 73/210 | Train loss: 4.0723 | Val loss: 3.8190 | Val RMSE: 1.9542\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "Epoch 74/210 | Train loss: 4.0367 | Val loss: 3.8099 | Val RMSE: 1.9519\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "Epoch 75/210 | Train loss: 4.0078 | Val loss: 3.7782 | Val RMSE: 1.9438\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "Epoch 76/210 | Train loss: 3.9619 | Val loss: 3.7725 | Val RMSE: 1.9423\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "Epoch 77/210 | Train loss: 3.9479 | Val loss: 3.7380 | Val RMSE: 1.9334\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "Epoch 78/210 | Train loss: 3.8988 | Val loss: 3.7269 | Val RMSE: 1.9305\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "Epoch 79/210 | Train loss: 3.8635 | Val loss: 3.7227 | Val RMSE: 1.9294\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "Epoch 80/210 | Train loss: 3.8145 | Val loss: 3.6850 | Val RMSE: 1.9196\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "Epoch 81/210 | Train loss: 3.7615 | Val loss: 3.6673 | Val RMSE: 1.9150\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "Epoch 82/210 | Train loss: 3.7266 | Val loss: 3.6286 | Val RMSE: 1.9049\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "Epoch 83/210 | Train loss: 3.6948 | Val loss: 3.6244 | Val RMSE: 1.9038\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "Epoch 84/210 | Train loss: 3.6578 | Val loss: 3.6119 | Val RMSE: 1.9005\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "Epoch 85/210 | Train loss: 3.6167 | Val loss: 3.5911 | Val RMSE: 1.8950\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "Epoch 86/210 | Train loss: 3.5933 | Val loss: 3.5848 | Val RMSE: 1.8934\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "\tValidation loss improved\n",
      "Epoch 87/210 | Train loss: 3.5608 | Val loss: 3.5724 | Val RMSE: 1.8901\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "\tValidation loss improved\n",
      "Epoch 88/210 | Train loss: 3.5275 | Val loss: 3.5719 | Val RMSE: 1.8899\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "\tValidation loss improved\n",
      "Epoch 89/210 | Train loss: 3.5252 | Val loss: 3.5752 | Val RMSE: 1.8908\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "Epoch 90/210 | Train loss: 3.4990 | Val loss: 3.5643 | Val RMSE: 1.8879\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n",
      "\tValidation loss improved\n",
      "Epoch 91/210 | Train loss: 4.1200 | Val loss: 4.0044 | Val RMSE: 2.0011\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "Epoch 92/210 | Train loss: 4.1780 | Val loss: 4.0248 | Val RMSE: 2.0062\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "Epoch 93/210 | Train loss: 4.2230 | Val loss: 3.9506 | Val RMSE: 1.9876\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "Epoch 94/210 | Train loss: 4.2173 | Val loss: 3.9995 | Val RMSE: 1.9999\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 95/210 | Train loss: 4.2038 | Val loss: 4.0490 | Val RMSE: 2.0122\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "Epoch 96/210 | Train loss: 4.1974 | Val loss: 4.1339 | Val RMSE: 2.0332\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 97/210 | Train loss: 4.1805 | Val loss: 4.0312 | Val RMSE: 2.0078\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "Epoch 98/210 | Train loss: 4.1711 | Val loss: 3.9457 | Val RMSE: 1.9864\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 99/210 | Train loss: 4.1504 | Val loss: 3.9876 | Val RMSE: 1.9969\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "Epoch 100/210 | Train loss: 4.1313 | Val loss: 3.9970 | Val RMSE: 1.9992\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "Epoch 101/210 | Train loss: 4.1205 | Val loss: 3.9054 | Val RMSE: 1.9762\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 102/210 | Train loss: 4.0843 | Val loss: 3.8462 | Val RMSE: 1.9612\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "Epoch 103/210 | Train loss: 4.0575 | Val loss: 3.8636 | Val RMSE: 1.9656\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "Epoch 104/210 | Train loss: 4.0169 | Val loss: 3.8545 | Val RMSE: 1.9633\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "Epoch 105/210 | Train loss: 3.9912 | Val loss: 3.7674 | Val RMSE: 1.9410\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "Epoch 106/210 | Train loss: 3.9598 | Val loss: 3.7467 | Val RMSE: 1.9356\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "Epoch 107/210 | Train loss: 3.9320 | Val loss: 3.7291 | Val RMSE: 1.9311\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "Epoch 108/210 | Train loss: 3.8905 | Val loss: 3.7035 | Val RMSE: 1.9245\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "Epoch 109/210 | Train loss: 3.8600 | Val loss: 3.7015 | Val RMSE: 1.9239\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "Epoch 110/210 | Train loss: 3.8123 | Val loss: 3.6639 | Val RMSE: 1.9141\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "Epoch 111/210 | Train loss: 3.7719 | Val loss: 3.6515 | Val RMSE: 1.9109\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "Epoch 112/210 | Train loss: 3.7165 | Val loss: 3.6275 | Val RMSE: 1.9046\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "Epoch 113/210 | Train loss: 3.6757 | Val loss: 3.6171 | Val RMSE: 1.9019\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "Epoch 114/210 | Train loss: 3.6544 | Val loss: 3.5835 | Val RMSE: 1.8930\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "Epoch 115/210 | Train loss: 3.6226 | Val loss: 3.5626 | Val RMSE: 1.8875\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "\tValidation loss improved\n",
      "Epoch 116/210 | Train loss: 3.5628 | Val loss: 3.5747 | Val RMSE: 1.8907\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "Epoch 117/210 | Train loss: 3.5431 | Val loss: 3.5547 | Val RMSE: 1.8854\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "\tValidation loss improved\n",
      "Epoch 118/210 | Train loss: 3.5232 | Val loss: 3.5564 | Val RMSE: 1.8859\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "Epoch 119/210 | Train loss: 3.5098 | Val loss: 3.5499 | Val RMSE: 1.8841\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "\tValidation loss improved\n",
      "Epoch 120/210 | Train loss: 3.5034 | Val loss: 3.5592 | Val RMSE: 1.8866\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n",
      "Epoch 121/210 | Train loss: 4.1113 | Val loss: 3.9886 | Val RMSE: 1.9971\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "Epoch 122/210 | Train loss: 4.1765 | Val loss: 4.0649 | Val RMSE: 2.0162\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "Epoch 123/210 | Train loss: 4.1944 | Val loss: 4.0782 | Val RMSE: 2.0195\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "Epoch 124/210 | Train loss: 4.1938 | Val loss: 3.9700 | Val RMSE: 1.9925\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 125/210 | Train loss: 4.1994 | Val loss: 4.0108 | Val RMSE: 2.0027\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "Epoch 126/210 | Train loss: 4.2034 | Val loss: 3.9431 | Val RMSE: 1.9857\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 127/210 | Train loss: 4.1890 | Val loss: 4.0175 | Val RMSE: 2.0044\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "Epoch 128/210 | Train loss: 4.1678 | Val loss: 3.9311 | Val RMSE: 1.9827\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 129/210 | Train loss: 4.1642 | Val loss: 3.9306 | Val RMSE: 1.9826\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "Epoch 130/210 | Train loss: 4.1344 | Val loss: 3.9774 | Val RMSE: 1.9943\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "Epoch 131/210 | Train loss: 4.1272 | Val loss: 3.8872 | Val RMSE: 1.9716\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 132/210 | Train loss: 4.0855 | Val loss: 3.8678 | Val RMSE: 1.9667\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "Epoch 133/210 | Train loss: 4.0538 | Val loss: 3.8386 | Val RMSE: 1.9592\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "Epoch 134/210 | Train loss: 4.0363 | Val loss: 3.7878 | Val RMSE: 1.9462\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "Epoch 135/210 | Train loss: 4.0074 | Val loss: 3.7835 | Val RMSE: 1.9451\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "Epoch 136/210 | Train loss: 3.9691 | Val loss: 3.7529 | Val RMSE: 1.9372\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "Epoch 137/210 | Train loss: 3.9320 | Val loss: 3.7332 | Val RMSE: 1.9321\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "Epoch 138/210 | Train loss: 3.8930 | Val loss: 3.6998 | Val RMSE: 1.9235\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "Epoch 139/210 | Train loss: 3.8665 | Val loss: 3.6766 | Val RMSE: 1.9175\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "Epoch 140/210 | Train loss: 3.8183 | Val loss: 3.6708 | Val RMSE: 1.9159\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "Epoch 141/210 | Train loss: 3.7868 | Val loss: 3.6470 | Val RMSE: 1.9097\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "Epoch 142/210 | Train loss: 3.7316 | Val loss: 3.6149 | Val RMSE: 1.9013\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "Epoch 143/210 | Train loss: 3.6916 | Val loss: 3.6062 | Val RMSE: 1.8990\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "Epoch 144/210 | Train loss: 3.6418 | Val loss: 3.5859 | Val RMSE: 1.8937\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "Epoch 145/210 | Train loss: 3.6011 | Val loss: 3.5702 | Val RMSE: 1.8895\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "Epoch 146/210 | Train loss: 3.5783 | Val loss: 3.5692 | Val RMSE: 1.8892\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "Epoch 147/210 | Train loss: 3.5619 | Val loss: 3.5565 | Val RMSE: 1.8859\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "Epoch 148/210 | Train loss: 3.5350 | Val loss: 3.5618 | Val RMSE: 1.8873\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "Epoch 149/210 | Train loss: 3.5117 | Val loss: 3.5546 | Val RMSE: 1.8854\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "Epoch 150/210 | Train loss: 3.5121 | Val loss: 3.5576 | Val RMSE: 1.8862\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n",
      "Epoch 151/210 | Train loss: 4.1055 | Val loss: 3.9687 | Val RMSE: 1.9922\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "Epoch 152/210 | Train loss: 4.1759 | Val loss: 3.9626 | Val RMSE: 1.9906\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "Epoch 153/210 | Train loss: 4.2164 | Val loss: 4.1609 | Val RMSE: 2.0398\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "Epoch 154/210 | Train loss: 4.2160 | Val loss: 4.0459 | Val RMSE: 2.0114\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 155/210 | Train loss: 4.2213 | Val loss: 4.0962 | Val RMSE: 2.0239\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "Epoch 156/210 | Train loss: 4.2051 | Val loss: 4.0132 | Val RMSE: 2.0033\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 157/210 | Train loss: 4.1985 | Val loss: 4.0618 | Val RMSE: 2.0154\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "Epoch 158/210 | Train loss: 4.1741 | Val loss: 3.9756 | Val RMSE: 1.9939\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 159/210 | Train loss: 4.1543 | Val loss: 3.9930 | Val RMSE: 1.9982\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "Epoch 160/210 | Train loss: 4.1369 | Val loss: 4.0024 | Val RMSE: 2.0006\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "Epoch 161/210 | Train loss: 4.1182 | Val loss: 3.8738 | Val RMSE: 1.9682\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 162/210 | Train loss: 4.1029 | Val loss: 3.8575 | Val RMSE: 1.9641\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "Epoch 163/210 | Train loss: 4.0595 | Val loss: 3.8736 | Val RMSE: 1.9681\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "Epoch 164/210 | Train loss: 4.0459 | Val loss: 3.8444 | Val RMSE: 1.9607\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "Epoch 165/210 | Train loss: 4.0096 | Val loss: 3.7901 | Val RMSE: 1.9468\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "Epoch 166/210 | Train loss: 3.9798 | Val loss: 3.7334 | Val RMSE: 1.9322\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "Epoch 167/210 | Train loss: 3.9373 | Val loss: 3.7211 | Val RMSE: 1.9290\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "Epoch 168/210 | Train loss: 3.8876 | Val loss: 3.6991 | Val RMSE: 1.9233\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "Epoch 169/210 | Train loss: 3.8632 | Val loss: 3.6875 | Val RMSE: 1.9203\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "Epoch 170/210 | Train loss: 3.8222 | Val loss: 3.6684 | Val RMSE: 1.9153\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "Epoch 171/210 | Train loss: 3.7665 | Val loss: 3.6579 | Val RMSE: 1.9126\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "Epoch 172/210 | Train loss: 3.7259 | Val loss: 3.6173 | Val RMSE: 1.9019\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "Epoch 173/210 | Train loss: 3.6732 | Val loss: 3.6126 | Val RMSE: 1.9007\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "Epoch 174/210 | Train loss: 3.6510 | Val loss: 3.6016 | Val RMSE: 1.8978\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "Epoch 175/210 | Train loss: 3.6029 | Val loss: 3.5768 | Val RMSE: 1.8913\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "Epoch 176/210 | Train loss: 3.5744 | Val loss: 3.5737 | Val RMSE: 1.8904\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "Epoch 177/210 | Train loss: 3.5534 | Val loss: 3.5758 | Val RMSE: 1.8910\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "Epoch 178/210 | Train loss: 3.5252 | Val loss: 3.5653 | Val RMSE: 1.8882\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "Epoch 179/210 | Train loss: 3.5277 | Val loss: 3.5673 | Val RMSE: 1.8887\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "Epoch 180/210 | Train loss: 3.5258 | Val loss: 3.5607 | Val RMSE: 1.8870\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n",
      "Epoch 181/210 | Train loss: 4.1137 | Val loss: 3.9743 | Val RMSE: 1.9936\n",
      "\tLR: [0.009972612215893682] | T_cur: 1\n",
      "Epoch 182/210 | Train loss: 4.1842 | Val loss: 4.0055 | Val RMSE: 2.0014\n",
      "\tLR: [0.009890748929868663] | T_cur: 2\n",
      "Epoch 183/210 | Train loss: 4.2051 | Val loss: 4.0049 | Val RMSE: 2.0012\n",
      "\tLR: [0.00975530705321762] | T_cur: 3\n",
      "Epoch 184/210 | Train loss: 4.2018 | Val loss: 3.9666 | Val RMSE: 1.9916\n",
      "\tLR: [0.009567770515484183] | T_cur: 4\n",
      "Epoch 185/210 | Train loss: 4.2230 | Val loss: 3.9922 | Val RMSE: 1.9980\n",
      "\tLR: [0.009330194006220302] | T_cur: 5\n",
      "Epoch 186/210 | Train loss: 4.2261 | Val loss: 4.0656 | Val RMSE: 2.0163\n",
      "\tLR: [0.00904518046337755] | T_cur: 6\n",
      "Epoch 187/210 | Train loss: 4.1910 | Val loss: 4.0538 | Val RMSE: 2.0134\n",
      "\tLR: [0.008715852554974233] | T_cur: 7\n",
      "Epoch 188/210 | Train loss: 4.1556 | Val loss: 3.9103 | Val RMSE: 1.9774\n",
      "\tLR: [0.008345818466491111] | T_cur: 8\n",
      "Epoch 189/210 | Train loss: 4.1534 | Val loss: 3.9881 | Val RMSE: 1.9970\n",
      "\tLR: [0.00793913236883622] | T_cur: 9\n",
      "Epoch 190/210 | Train loss: 4.1318 | Val loss: 4.0117 | Val RMSE: 2.0029\n",
      "\tLR: [0.007500250000000001] | T_cur: 10\n",
      "Epoch 191/210 | Train loss: 4.1063 | Val loss: 3.9435 | Val RMSE: 1.9858\n",
      "\tLR: [0.007033979847057465] | T_cur: 11\n",
      "Epoch 192/210 | Train loss: 4.0960 | Val loss: 3.8563 | Val RMSE: 1.9637\n",
      "\tLR: [0.00654543046337755] | T_cur: 12\n",
      "Epoch 193/210 | Train loss: 4.0655 | Val loss: 3.7843 | Val RMSE: 1.9453\n",
      "\tLR: [0.006039954498243388] | T_cur: 13\n",
      "Epoch 194/210 | Train loss: 4.0406 | Val loss: 3.8095 | Val RMSE: 1.9518\n",
      "\tLR: [0.005523090052106635] | T_cur: 14\n",
      "Epoch 195/210 | Train loss: 4.0130 | Val loss: 3.7860 | Val RMSE: 1.9458\n",
      "\tLR: [0.0050005000000000015] | T_cur: 15\n",
      "Epoch 196/210 | Train loss: 3.9738 | Val loss: 3.7607 | Val RMSE: 1.9393\n",
      "\tLR: [0.0044779099478933675] | T_cur: 16\n",
      "Epoch 197/210 | Train loss: 3.9271 | Val loss: 3.7393 | Val RMSE: 1.9337\n",
      "\tLR: [0.003961045501756613] | T_cur: 17\n",
      "Epoch 198/210 | Train loss: 3.8980 | Val loss: 3.7109 | Val RMSE: 1.9264\n",
      "\tLR: [0.0034555695366224513] | T_cur: 18\n",
      "Epoch 199/210 | Train loss: 3.8499 | Val loss: 3.6727 | Val RMSE: 1.9164\n",
      "\tLR: [0.0029670201529425383] | T_cur: 19\n",
      "Epoch 200/210 | Train loss: 3.8249 | Val loss: 3.6772 | Val RMSE: 1.9176\n",
      "\tLR: [0.0025007500000000017] | T_cur: 20\n",
      "Epoch 201/210 | Train loss: 3.7793 | Val loss: 3.6410 | Val RMSE: 1.9082\n",
      "\tLR: [0.0020618676311637816] | T_cur: 21\n",
      "Epoch 202/210 | Train loss: 3.7381 | Val loss: 3.6360 | Val RMSE: 1.9068\n",
      "\tLR: [0.00165518153350889] | T_cur: 22\n",
      "Epoch 203/210 | Train loss: 3.6808 | Val loss: 3.6166 | Val RMSE: 1.9017\n",
      "\tLR: [0.0012851474450257686] | T_cur: 23\n",
      "Epoch 204/210 | Train loss: 3.6602 | Val loss: 3.6036 | Val RMSE: 1.8983\n",
      "\tLR: [0.0009558195366224509] | T_cur: 24\n",
      "Epoch 205/210 | Train loss: 3.6174 | Val loss: 3.5834 | Val RMSE: 1.8930\n",
      "\tLR: [0.0006708059937796987] | T_cur: 25\n",
      "Epoch 206/210 | Train loss: 3.5811 | Val loss: 3.5618 | Val RMSE: 1.8873\n",
      "\tLR: [0.0004332294845158165] | T_cur: 26\n",
      "Epoch 207/210 | Train loss: 3.5528 | Val loss: 3.5676 | Val RMSE: 1.8888\n",
      "\tLR: [0.00024569294678237997] | T_cur: 27\n",
      "Epoch 208/210 | Train loss: 3.5526 | Val loss: 3.5764 | Val RMSE: 1.8911\n",
      "\tLR: [0.00011025107013133847] | T_cur: 28\n",
      "Epoch 209/210 | Train loss: 3.5321 | Val loss: 3.5706 | Val RMSE: 1.8896\n",
      "\tLR: [2.8387784106317135e-05] | T_cur: 29\n",
      "Epoch 210/210 | Train loss: 3.5139 | Val loss: 3.5735 | Val RMSE: 1.8904\n",
      "\tLR: [0.01] | T_cur: 0\n",
      "\tSnapshot taken\n"
     ]
    }
   ],
   "source": [
    "### Model setup\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import os\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [512, 512, 256, 128, 64]\n",
    "\n",
    "model = MLPRegressor(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dims=hidden_dims,\n",
    "    learning_rate=1e-2,\n",
    "    use_snapshots=True,\n",
    "    cycle_length=30,\n",
    "    cycle_mult=1,\n",
    "    batch_normalization=True,\n",
    "    weight_decay=0.01,\n",
    "    dropout_rate=0.4,\n",
    "    early_stopping_patience=150,\n",
    "    activation_fn=nn.ReLU()\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = model.get_optimizer()\n",
    "scheduler = model.get_scheduler(optimizer)\n",
    "\n",
    "# create a dir for saving the best model - useful for snapshot ensembles with multiple models present\n",
    "save_dir = 'best_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, criterion, device, epochs=100):\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    snapshots = []\n",
    "\n",
    "    for epoch in range (1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # snapshot ensembling\n",
    "        isSnapshot = False\n",
    "        if model.use_snapshots and scheduler.T_cur == 0 and epoch != 0:\n",
    "            snapshots.append(model.state_dict().copy())\n",
    "            torch.save(model.state_dict(), f'{save_dir}/snapshot_{len(snapshots)}.pth')\n",
    "            isSnapshot = True\n",
    "\n",
    "        # eval phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                if model.use_snapshots and len(snapshots) > 0:\n",
    "                    ens_predictions = model.ensemble_predict(inputs, snapshots, device)\n",
    "                    ens_predictions.append(model(inputs).squeeze())\n",
    "\n",
    "                    # Adds current model as part of snapshot eval\n",
    "                    outputs = torch.mean(torch.stack(ens_predictions), dim=0)\n",
    "\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                else:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Early stopping\n",
    "        isEpochImprove = False\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f'{save_dir}/best_model.pth')\n",
    "            isEpochImprove = True\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= model.early_stopping_patience:\n",
    "                print(f'Stopping early with best val loss {avg_val_loss}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs} | Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f} | Val RMSE: {np.sqrt(avg_val_loss):.4f}')\n",
    "        print(f'\\tLR: {scheduler.get_last_lr()} | T_cur: {scheduler.T_cur}')\n",
    "        if isSnapshot:\n",
    "            print(f'\\tSnapshot taken')\n",
    "        if isEpochImprove:\n",
    "            print(f'\\tValidation loss improved')\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, scheduler, model.criterion, device, epochs=210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a snapshotted model for inference, just initialize MLP Regressor classes for all snapshots, and load a snapshot state dict into each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference using saved models\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for model_path in os.listdir(save_dir):\n",
    "    model = MLPRegressor(\n",
    "        input_dim=input_dim, \n",
    "        hidden_dims=hidden_dims,\n",
    "        learning_rate=1e-2,\n",
    "        use_snapshots=True,\n",
    "        cycle_length=30,\n",
    "        cycle_mult=1,\n",
    "        batch_normalization=True,\n",
    "        weight_decay=0.01,\n",
    "        dropout_rate=0.4,\n",
    "        early_stopping_patience=150,\n",
    "        activation_fn=nn.ReLU()\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{save_dir}/{model_path}'))\n",
    "    model.to(device)\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: 1.8869935274124146\n",
      "Model 1: 1.8865877389907837\n",
      "Model 2: 1.8908019065856934\n",
      "Model 3: 1.8874437808990479\n",
      "Model 4: 1.8957056999206543\n",
      "Model 5: 1.8883732557296753\n",
      "Model 6: 1.911879301071167\n",
      "Model 7: 1.8845239877700806\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(model_list):\n",
    "    with torch.no_grad():\n",
    "        test_model = model_list[i]\n",
    "        test_model.eval()\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for inputs, targets in val_loader:\n",
    "            y_pred_batch = test_model(inputs).squeeze().cpu().numpy()\n",
    "            y_true_batch = targets.cpu().numpy()\n",
    "            y_pred_list.extend(y_pred_batch)\n",
    "            y_true_list.extend(y_true_batch)\n",
    "\n",
    "        y_pred = np.array(y_pred_list)\n",
    "        y_true = np.array(y_true_list)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    print(f\"Model {i}: {np.sqrt(mean_squared_error(y_true, y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean prediction: 1.8821855783462524\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for i, model in enumerate(model_list):\n",
    "        test_model = model\n",
    "        test_model.eval()\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for inputs, targets in val_loader:\n",
    "            y_pred_batch = test_model(inputs).squeeze().cpu().numpy()\n",
    "            y_true_batch = targets.cpu().numpy()\n",
    "            y_pred_list.extend(y_pred_batch)\n",
    "            y_true_list.extend(y_true_batch)\n",
    "        \n",
    "        y_pred = np.array(y_pred_list)\n",
    "        y_true = np.array(y_true_list)\n",
    "\n",
    "        preds.append(y_pred)\n",
    "\n",
    "    y_pred_val = np.mean(preds, axis=0)\n",
    "\n",
    "print(f\"Combined mean prediction: {np.sqrt(mean_squared_error(y_true, y_pred_val))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np = X_test.to_numpy().astype(np.float32)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test_np).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for i, model in enumerate(model_list):\n",
    "        test_model = model\n",
    "        test_model.eval()\n",
    "\n",
    "        y_pred = test_model(X_test_tensor).squeeze().cpu().numpy()\n",
    "        \n",
    "        preds.extend(y_pred)\n",
    "    \n",
    "    preds = np.array(preds)\n",
    "    y_pred = np.mean(preds, axis=0)\n",
    "\n",
    "\n",
    "df_subm = pd.read_csv(\n",
    "    \"brist1d/sample_submission.csv\",\n",
    "    index_col='id',\n",
    ")\n",
    "\n",
    "df_subm['bg+1:00'] = y_pred\n",
    "df_subm.to_csv('submission_mlp_regressor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
